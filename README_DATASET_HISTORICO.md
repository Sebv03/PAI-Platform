# ğŸ“Š Dataset HistÃ³rico de la Plataforma

Este documento explica cÃ³mo obtener y usar el dataset histÃ³rico real que usa la plataforma PAI para entrenar el modelo ML.

## ğŸ“¦ Archivo Exportado

**Archivo**: `datasets/historical_dataset.csv`

**CaracterÃ­sticas**:
- Datos reales de la base de datos de la plataforma
- Misma estructura que usa el ML service
- Misma query que ejecuta el modelo para entrenar
- Datos histÃ³ricos generados por `populate_paes_historical_data.py`

## ğŸš€ CÃ³mo Exportar el Dataset

Ejecuta el script desde la raÃ­z del proyecto:

```bash
python export_historical_data_to_csv.py
```

**Requisitos**:
- La base de datos debe estar corriendo
- Debe tener datos histÃ³ricos (ejecutar `backend/populate_paes_historical_data.py` primero)
- Debe tener acceso a la base de datos configurada en `backend/app/core/config.py`

## ğŸ“‹ Estructura del CSV

El CSV contiene las mismas columnas que usa el ML service:

| Columna | DescripciÃ³n | Tipo |
|---------|-------------|------|
| `task_id` | ID de la tarea | int |
| `course_id` | ID del curso | int |
| `student_id` | ID del estudiante | int |
| `task_created_at` | Fecha de creaciÃ³n de la tarea | datetime |
| `due_date` | Fecha lÃ­mite de entrega | datetime |
| `enrollment_date` | Fecha de inscripciÃ³n del estudiante | datetime |
| `submission_id` | ID de la entrega (NaN si no entregÃ³) | float/NaN |
| `submitted_at` | Fecha de entrega (NaN si no entregÃ³) | datetime/NaN |
| `grade` | Nota recibida (NaN si no entregÃ³ o no calificado) | float/NaN |

## ğŸ“ Uso en Google Colab

1. **Exportar el CSV** (desde tu mÃ¡quina local):
   ```bash
   python export_historical_data_to_csv.py
   ```

2. **Subir el CSV a Colab**:
   ```python
   from google.colab import files
   uploaded = files.upload()  # Selecciona historical_dataset_with_profiles.csv
   df = pd.read_csv('historical_dataset_with_profiles.csv')
   ```

3. **O cargar desde GitHub** (si lo subiste):
   ```python
   url = "https://raw.githubusercontent.com/Sebv03/PAI-Platform/main/datasets/historical_dataset_with_profiles.csv"
   df = pd.read_csv(url)
   ```

4. **Seguir la guÃ­a**: Ver `GUIA_COLAB_MODELO_COMPLETO.md` para instrucciones completas

## ğŸ“ˆ Datos del Dataset

### EstadÃ­sticas Actuales

- **Total registros**: ~351 (varÃ­a segÃºn datos en BD)
- **Estudiantes Ãºnicos**: ~26
- **Cursos Ãºnicos**: ~5
- **Tareas Ãºnicas**: ~21
- **Tasa de entrega**: ~70%
- **Notas promedio**: ~4.75 (escala 1-7)

### Origen de los Datos

Los datos provienen de:
- Usuarios creados por `backend/populate_paes_historical_data.py`
- Cursos PAES: NÃºmeros, Ãlgebra y Funciones, GeometrÃ­a, ComprensiÃ³n Lectora, etc.
- Estudiantes de 1Â° a 4Â° medio (14-18 aÃ±os)
- Tareas distribuidas a lo largo del perÃ­odo
- Entregas con diferentes patrones de comportamiento
- Calificaciones basadas en rendimiento y puntualidad (escala 1.0-7.0)

## âœ… ValidaciÃ³n

Este CSV es **exactamente** el mismo que usa:

1. âœ… El ML service para entrenar el modelo (`data_service.py`)
2. âœ… La misma query SQL que ejecuta el backend
3. âœ… Los mismos datos que verÃ­as en la plataforma

## ğŸ”„ Actualizar el Dataset

Si agregas mÃ¡s datos a la plataforma:

1. Ejecuta `export_historical_data_to_csv.py` nuevamente
2. El CSV se actualizarÃ¡ con los nuevos datos
3. Puedes reentrenar el modelo con los datos actualizados

## ğŸ“ ComparaciÃ³n con Test Dataset

Para mÃ¡s detalles sobre el uso en Google Colab, consulta `GUIA_COLAB_MODELO_COMPLETO.md`

## ğŸ¯ Resultados Esperados

Con este dataset histÃ³rico, el modelo deberÃ­a alcanzar mÃ©tricas similares a las que obtienes en la plataforma local, ya que son los mismos datos.

## âš ï¸ Notas Importantes

- El dataset puede variar si agregas/eliminas datos en la BD
- Las fechas estÃ¡n en formato UTC con timezone
- Los valores NaN representan datos faltantes (no entregado, no calificado)
- El CSV estÃ¡ codificado en UTF-8

## ğŸ” Query SQL Usada

El script usa la misma query que el ML service:

```sql
SELECT 
    t.id as task_id,
    t.course_id,
    t.due_date,
    t.created_at as task_created_at,
    e.student_id,
    e.enrollment_date,
    s.id as submission_id,
    s.submitted_at,
    s.grade
FROM tasks t
INNER JOIN enrollments e ON t.course_id = e.course_id
LEFT JOIN submissions s ON s.task_id = t.id AND s.student_id = e.student_id
ORDER BY e.student_id, t.course_id, t.due_date
```

---

**Â¡Listo! ğŸ‰ Ahora tienes el mismo dataset que usa la plataforma para entrenar el modelo.**

